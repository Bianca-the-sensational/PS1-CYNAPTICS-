{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9c9xA6J0FfC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "import torchaudio\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# MOUNT ON GOOGLE DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive' , force_remount = True)\n",
        "\n",
        "# ---- SEED ----\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# ---- PATHS ----\n",
        "base_path = '/content/drive/MyDrive/the-frequency-quest'\n",
        "train_dir = os.path.join(base_path, 'train' , 'train')\n",
        "test_dir = os.path.join(base_path, 'test' , 'test')\n",
        "\n",
        "# ---- PARAMETERS ----\n",
        "N_MELS = 64\n",
        "TARGET_SR = 22050\n",
        "MAX_FRAMES = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15\n",
        "# DRECTORY TO STORE MEL SPECTROGRAMS FOR EACH AUDIO FILE\n",
        "CACHE_DIR = os.path.join(base_path, \"cache_mel\")\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---- CLASSES ----\n",
        "classes = sorted(d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d)))\n",
        "class_to_index = {cls: idx for idx, cls in enumerate(classes)}\n",
        "\n",
        "file_paths, labels = [], []\n",
        "for cls in classes:\n",
        "    cls_path = os.path.join(train_dir, cls)\n",
        "    for f in os.listdir(cls_path):\n",
        "        if f.endswith('.wav'):\n",
        "            file_paths.append(os.path.join(cls_path, f))\n",
        "            labels.append(class_to_index[cls])\n",
        "\n",
        "# SPLIT THE FILES AND CORRESPONDING LABELS TO TRAIN AND TEST DATA\n",
        "train_files, val_files, train_labels, val_labels = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---- DATASET ----\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, files, labels=None, train=True):\n",
        "        self.files = files\n",
        "        self.labels = labels\n",
        "        self.train = train\n",
        "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=TARGET_SR, n_fft=1024, hop_length=512, n_mels=N_MELS\n",
        "        )\n",
        "        self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=15)\n",
        "        self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
        "\n",
        "    def _process(self, path):\n",
        "        cache_path = os.path.join(CACHE_DIR, os.path.basename(path) + \".pt\")\n",
        "        if os.path.exists(cache_path):\n",
        "            mel = torch.load(cache_path)\n",
        "        else:\n",
        "            audio, sr = torchaudio.load(path)\n",
        "            # RESAMPLE\n",
        "            if sr != TARGET_SR:\n",
        "                audio = torchaudio.transforms.Resample(sr, TARGET_SR)(audio)\n",
        "            # CONVERT STEREO TO MONO(>1 -> 1)\n",
        "            if audio.shape[0] > 1:\n",
        "                audio = audio.mean(dim=0, keepdim=True)\n",
        "            # GET MEL SPECTROGRAM FROM WAVEFORM\n",
        "            mel = self.mel_spec(audio)\n",
        "            # TO NORMALIZE THE MEL SPECTROGRAM TENSOR FROM 0-1\n",
        "            mel = (mel - mel.mean()) / (mel.std() + 1e-9)\n",
        "            # PADDING\n",
        "            if mel.shape[2] < MAX_FRAMES:\n",
        "              pad = MAX_FRAMES - mel.shape[2]\n",
        "              mel = F.pad(0 , pad)\n",
        "            else:\n",
        "              mel = mel[: , : , :MAX_FRAMES]\n",
        "            torch.save(mel, cache_path)\n",
        "        return mel\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mel = self._process(self.files[idx])\n",
        "        if self.train:\n",
        "            mel = self.freq_mask(mel)\n",
        "            mel = self.time_mask(mel)\n",
        "        mel = mel.repeat(3, 1, 1)\n",
        "        mel = F.interpolate(mel.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
        "        if self.labels is not None:\n",
        "            return mel, self.labels[idx]\n",
        "        return mel\n",
        "\n",
        "# ---- DATALOADERS ----\n",
        "train_ds = AudioDataset(train_files, train_labels, train=True)\n",
        "val_ds = AudioDataset(val_files, val_labels, train=False)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# ---- MODEL ----\n",
        "# FEATURE EXTRACTION BY PRETRAINED MODEL - DONE\n",
        "model = models.resnet34(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# FEATURE CLASSIFICITATION\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(model.fc.in_features, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, len(classes))\n",
        ")\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n",
        "\n",
        "# ---- TRAINING LOOP ----\n",
        "best_acc = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for xb, yb in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    avg_loss = running_loss / len(train_dl)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_dl:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = model(xb)\n",
        "            _, pred_labels = preds.max(1)\n",
        "            correct += (pred_labels == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    acc = correct / total\n",
        "    scheduler.step(avg_loss)\n",
        "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Val Acc: {acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "print(f\"Best Validation Accuracy: {best_acc:.4f}\")\n",
        "\n",
        "# ---- TEST ----\n",
        "test_files = [os.path.join(test_dir, f) for f in os.listdir(test_dir) if f.endswith('.wav')]\n",
        "print(len(test_files))\n",
        "test_ds = AudioDataset(test_files, train=False)\n",
        "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# LOAD THE BEST MODEL FOR THE TESTING PROCESS\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for xb in tqdm(test_dl, desc=\"Predicting\"):\n",
        "        xb = xb.to(device)\n",
        "        preds = model(xb)\n",
        "        _, pred_labels = preds.max(1)\n",
        "        predictions.extend(pred_labels.cpu().numpy())\n",
        "\n",
        "idx_to_class = {v: k for k, v in class_to_index.items()}\n",
        "predicted_classes = [idx_to_class[i] for i in predictions]\n",
        "\n",
        "submission = pd.DataFrame({'ID': [os.path.basename(f) for f in test_files], 'Class': predicted_classes})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file created: submission.csv\")\n"
      ]
    }
  ]
}